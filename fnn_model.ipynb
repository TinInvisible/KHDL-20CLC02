{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import preprocess_model as pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Flatten, Concatenate, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the CSV file\n",
    "df = pre.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre.X_col = ['School enrollment, primary', 'School enrollment, tertiary', 'Primary completion rate', 'Year']\n",
    "# pre.Country = ['pre.Country']\n",
    "# y_target = 'Literacy rate'\n",
    "\n",
    "# # Drop rows with missing target values\n",
    "# df.dropna(subset=[y_target], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split the data into training, validation, and test sets\n",
    "# pre.X_train, pre.X_temp, pre.y_train, pre.y_temp = train_test_split(df[pre.X_col + pre.Country], df[y_target], test_size=0.4, random_state=42)\n",
    "# pre.X_val, pre.X_test, pre.y_val, pre.y_test = train_test_split(pre.X_temp, pre.y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# print(\"Shapes after splitting:\")\n",
    "# print(\"pre.X_train:\", pre.X_train.shape, \"pre.y_train:\", pre.y_train.shape)\n",
    "# print(\"pre.X_val:\", pre.X_val.shape, \"pre.y_val:\", pre.y_val.shape)\n",
    "# print(\"pre.X_test:\", pre.X_test.shape, \"pre.y_test:\", pre.y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Standardize the numerical features\n",
    "# pre.scaler = StandardScaler()\n",
    "# pre.X_train[pre.X_col] = pre.scaler.fit_transform(pre.X_train[pre.X_col])\n",
    "# pre.X_val[pre.X_col] = pre.scaler.transform(pre.X_val[pre.X_col])\n",
    "# pre.X_test[pre.X_col] = pre.scaler.transform(pre.X_test[pre.X_col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Preprocess categorical features\n",
    "# label_encoders = {}\n",
    "# for country in pre.Country:\n",
    "#     le = LabelEncoder()\n",
    "#     pre.X_train[country] = le.fit_transform(pre.X_train[country])\n",
    "#     pre.X_val[country] = le.transform(pre.X_val[country])\n",
    "#     pre.X_test[country] = le.transform(pre.X_test[country])\n",
    "#     label_encoders[country] = le\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the learning rate\n",
    "learning_rate = 0.0001  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(lr\u001b[38;5;241m=\u001b[39mlearning_rate), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m#soft-max reg\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Train the model for more epochs\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\u001b[43m[\u001b[49m\u001b[43mpre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcountry\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcountry\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCountry\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m [pre\u001b[38;5;241m.\u001b[39mX_train[pre\u001b[38;5;241m.\u001b[39mX_col]],\n\u001b[0;32m     36\u001b[0m                     pre\u001b[38;5;241m.\u001b[39my_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[0;32m     37\u001b[0m                     validation_data\u001b[38;5;241m=\u001b[39m([pre\u001b[38;5;241m.\u001b[39mX_val[country] \u001b[38;5;28;01mfor\u001b[39;00m country \u001b[38;5;129;01min\u001b[39;00m pre\u001b[38;5;241m.\u001b[39mCountry] \u001b[38;5;241m+\u001b[39m [pre\u001b[38;5;241m.\u001b[39mX_val[pre\u001b[38;5;241m.\u001b[39mX_col]], pre\u001b[38;5;241m.\u001b[39my_val),\n\u001b[0;32m     38\u001b[0m                     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n\u001b[0;32m     41\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate([pre\u001b[38;5;241m.\u001b[39mX_test[country] \u001b[38;5;28;01mfor\u001b[39;00m country \u001b[38;5;129;01min\u001b[39;00m pre\u001b[38;5;241m.\u001b[39mCountry] \u001b[38;5;241m+\u001b[39m [pre\u001b[38;5;241m.\u001b[39mX_test[pre\u001b[38;5;241m.\u001b[39mX_col]], pre\u001b[38;5;241m.\u001b[39my_test)\n",
      "Cell \u001b[1;32mIn[27], line 35\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     32\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(lr\u001b[38;5;241m=\u001b[39mlearning_rate), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m#soft-max reg\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Train the model for more epochs\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit([\u001b[43mpre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcountry\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m country \u001b[38;5;129;01min\u001b[39;00m pre\u001b[38;5;241m.\u001b[39mCountry] \u001b[38;5;241m+\u001b[39m [pre\u001b[38;5;241m.\u001b[39mX_train[pre\u001b[38;5;241m.\u001b[39mX_col]],\n\u001b[0;32m     36\u001b[0m                     pre\u001b[38;5;241m.\u001b[39my_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[0;32m     37\u001b[0m                     validation_data\u001b[38;5;241m=\u001b[39m([pre\u001b[38;5;241m.\u001b[39mX_val[country] \u001b[38;5;28;01mfor\u001b[39;00m country \u001b[38;5;129;01min\u001b[39;00m pre\u001b[38;5;241m.\u001b[39mCountry] \u001b[38;5;241m+\u001b[39m [pre\u001b[38;5;241m.\u001b[39mX_val[pre\u001b[38;5;241m.\u001b[39mX_col]], pre\u001b[38;5;241m.\u001b[39my_val),\n\u001b[0;32m     38\u001b[0m                     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n\u001b[0;32m     41\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate([pre\u001b[38;5;241m.\u001b[39mX_test[country] \u001b[38;5;28;01mfor\u001b[39;00m country \u001b[38;5;129;01min\u001b[39;00m pre\u001b[38;5;241m.\u001b[39mCountry] \u001b[38;5;241m+\u001b[39m [pre\u001b[38;5;241m.\u001b[39mX_test[pre\u001b[38;5;241m.\u001b[39mX_col]], pre\u001b[38;5;241m.\u001b[39my_test)\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the model\n",
    "input_layers = []\n",
    "embedding_layers = []\n",
    "\n",
    "for country in pre.Country:\n",
    "    input_layer = Input(shape=(1,), name=country)\n",
    "    embedding_layer = Embedding(input_dim=len(df[country].unique()), output_dim=10)(input_layer)\n",
    "    flatten_layer = Flatten()(embedding_layer)\n",
    "    input_layers.append(input_layer)\n",
    "    embedding_layers.append(flatten_layer)\n",
    "\n",
    "input = Input(shape=(len(pre.X_col),), name='input')\n",
    "embedding_layers.append(input)\n",
    "\n",
    "# Concatenate all input layers\n",
    "concatenated = Concatenate()(embedding_layers)\n",
    "\n",
    "# Fully connected layers with dropout for regularization\n",
    "x = Dense(256, activation='relu')(concatenated)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "output_layer = Dense(1, activation='linear', name='output')(x)\n",
    "\n",
    "# Model\n",
    "model = Model(inputs=input_layers + [input], outputs=output_layer)\n",
    "\n",
    "# Compile the model with Mean Squared Error loss and the specified learning rate\n",
    "model.compile(optimizer=Adam(lr=learning_rate), loss='mean_squared_error') #soft-max reg\n",
    "\n",
    "# Train the model for more epochs\n",
    "history = model.fit([pre.X_train[country] for country in pre.Country] + [pre.X_train[pre.X_col]],\n",
    "                    pre.y_train, epochs=200, batch_size=64,\n",
    "                    validation_data=([pre.X_val[country] for country in pre.Country] + [pre.X_val[pre.X_col]], pre.y_val),\n",
    "                    verbose=1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss = model.evaluate([pre.X_test[country] for country in pre.Country] + [pre.X_test[pre.X_col]], pre.y_test)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "\n",
    "# Plot the training history\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      School enrollment, primary  School enrollment, tertiary   \n",
      "1                     100.911263                    28.845509  \\\n",
      "7                     106.182419                    28.845509   \n",
      "11                    100.290298                     3.755610   \n",
      "30                     96.620567                    53.306129   \n",
      "33                    101.619812                    53.590759   \n",
      "...                          ...                          ...   \n",
      "1263                  117.423347                    28.506929   \n",
      "1273                   97.419968                    21.506201   \n",
      "1282                  112.737328                     9.135250   \n",
      "1283                  114.000847                    10.333910   \n",
      "1306                   80.937019                     8.826330   \n",
      "\n",
      "      Primary completion rate  Year      Country  \n",
      "1                   96.636715  2021  Afghanistan  \n",
      "7                   96.636715  2015  Afghanistan  \n",
      "11                  96.636715  2011  Afghanistan  \n",
      "30                 101.986748  2020      Armenia  \n",
      "33                 102.763428  2017      Armenia  \n",
      "...                       ...   ...          ...  \n",
      "1263                96.636715  2019      Vietnam  \n",
      "1273                96.636715  2009      Vietnam  \n",
      "1282               101.050873  2000      Vietnam  \n",
      "1283               100.163330  1999      Vietnam  \n",
      "1306                55.833328  2004  Yemen, Rep.  \n",
      "\n",
      "[267 rows x 5 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: 'Japan'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\_encode.py:224\u001b[0m, in \u001b[0;36m_encode\u001b[1;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[0;32m    225\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\_encode.py:164\u001b[0m, in \u001b[0;36m_map_to_integer\u001b[1;34m(values, uniques)\u001b[0m\n\u001b[0;32m    163\u001b[0m table \u001b[39m=\u001b[39m _nandict({val: i \u001b[39mfor\u001b[39;00m i, val \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(uniques)})\n\u001b[1;32m--> 164\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray([table[v] \u001b[39mfor\u001b[39;49;00m v \u001b[39min\u001b[39;49;00m values])\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\_encode.py:164\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    163\u001b[0m table \u001b[39m=\u001b[39m _nandict({val: i \u001b[39mfor\u001b[39;00m i, val \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(uniques)})\n\u001b[1;32m--> 164\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray([table[v] \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m values])\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\_encode.py:158\u001b[0m, in \u001b[0;36m_nandict.__missing__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnan_value\n\u001b[1;32m--> 158\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Japan'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Preprocess categorical features\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cat_feature \u001b[38;5;129;01min\u001b[39;00m categorical_features:\n\u001b[1;32m---> 19\u001b[0m     new_data[cat_feature] \u001b[38;5;241m=\u001b[39m \u001b[43mlabel_encoders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcat_feature\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcat_feature\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# # Make predictions on the new data\u001b[39;00m\n\u001b[0;32m     22\u001b[0m predictions_future \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict([new_data[cat_feature] \u001b[38;5;28;01mfor\u001b[39;00m cat_feature \u001b[38;5;129;01min\u001b[39;00m categorical_features] \u001b[38;5;241m+\u001b[39m [new_data[numerical_features]])\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:139\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[39mif\u001b[39;00m _num_samples(y) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    137\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray([])\n\u001b[1;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m _encode(y, uniques\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclasses_)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\_encode.py:226\u001b[0m, in \u001b[0;36m_encode\u001b[1;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[39mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[0;32m    225\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 226\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my contains previously unseen labels: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    227\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    228\u001b[0m     \u001b[39mif\u001b[39;00m check_unknown:\n",
      "\u001b[1;31mValueError\u001b[0m: y contains previously unseen labels: 'Japan'"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "test = ['School enrollment, primary', 'School enrollment, tertiary', 'Primary completion rate', 'Year', 'Country']\n",
    "print(df[test])\n",
    "new_data = pd.DataFrame({\n",
    "    'School enrollment, primary': [100.911263,100.911262512207],\n",
    "    'School enrollment, tertiary': [28.845509  ,28.8455085754395],\n",
    "    'Primary completion rate': [96.636715, 96.63671493530276],\n",
    "    'Year': [2023,2023],\n",
    "    'Country': [\"Afghanistan\", \"Japan\"],\n",
    "    # other columns as needed\n",
    "})\n",
    "# 100.911262512207,28.8455085754395,96.63671493530276\n",
    "# Preprocess the new data similar to the training data\n",
    "# Standardize numerical features\n",
    "new_data[pre.X_col] = pre.scaler.transform(new_data[pre.X_col])\n",
    "\n",
    "# Preprocess categorical features\n",
    "for country in pre.Country:\n",
    "    new_data[country] = pre.label_encoders[country].transform(new_data[country])\n",
    "\n",
    "# # Make predictions on the new data\n",
    "predictions_future = model.predict([new_data[country] for country in pre.Country] + [new_data[pre.X_col]])\n",
    "print(predictions_future)\n",
    "\n",
    "# predictions = model.predict([pre.X_test[country] for country in pre.Country] + [pre.X_test[pre.X_col]])\n",
    "# print(pre.y_test)\n",
    "# print(predictions)\n",
    "# import seaborn as sns\n",
    "\n",
    "# sns.kdeplot(pre.y_test, label='Actual', shade=True)\n",
    "# sns.kdeplot(predictions.flatten(), label='Predicted', shade=True)\n",
    "# plt.xlabel('Literacy rate')\n",
    "# plt.title('Distribution of Actual vs. Predicted Literacy rate')\n",
    "# plt.show()\n",
    "# residuals = pre.y_test - predictions.flatten()\n",
    "\n",
    "# # plt.scatter(pre.y_test, residuals)\n",
    "# # plt.axhline(y=0, color='red', linestyle='--')\n",
    "# # plt.xlabel('Actual Literacy rate')\n",
    "# # plt.ylabel('Residuals')\n",
    "# # plt.title('Residual Plot')\n",
    "# # plt.show()\n",
    "\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# mse = mean_squared_error(pre.y_test, predictions)\n",
    "# mae = mean_absolute_error(pre.y_test, predictions)\n",
    "\n",
    "# r2 = r2_score(pre.y_test, predictions)\n",
    "# print(f'Mean Squared Error: {mse}')\n",
    "# print(f'Mean Absolute Error: {mae}')\n",
    "# print(f'R-squared: {r2}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
