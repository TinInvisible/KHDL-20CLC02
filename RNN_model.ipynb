{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import preprocess_model\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Flatten, Concatenate, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import LSTM, Reshape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/new_data_asia.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = ['School enrollment, primary', 'School enrollment, tertiary', 'Primary completion rate', 'Year']\n",
    "categorical_features = ['Country']\n",
    "target = 'Literacy rate'\n",
    "\n",
    "# Drop rows with missing target values\n",
    "df.dropna(subset=[target], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, SimpleRNN, Dense, Concatenate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  ----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "4/4 [==============================] - 2s 120ms/step - loss: 8921.1826 - val_loss: 9169.8701\n",
      "Epoch 2/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 8894.3057 - val_loss: 9143.6152\n",
      "Epoch 3/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 8865.4502 - val_loss: 9113.2412\n",
      "Epoch 4/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 8831.8477 - val_loss: 9067.7988\n",
      "Epoch 5/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 8768.9043 - val_loss: 8904.1172\n",
      "Epoch 6/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 8381.5918 - val_loss: 6848.0229\n",
      "Epoch 7/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 4065.0537 - val_loss: 871.3710\n",
      "Epoch 8/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 783.1224 - val_loss: 1167.0432\n",
      "Epoch 9/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 650.3815 - val_loss: 812.9379\n",
      "Epoch 10/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 456.8484 - val_loss: 618.0742\n",
      "Epoch 11/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 431.4966 - val_loss: 258.1475\n",
      "Epoch 12/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 232.4671 - val_loss: 189.0959\n",
      "Epoch 13/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 178.4070 - val_loss: 131.8784\n",
      "Epoch 14/200\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 162.7400 - val_loss: 143.0381\n",
      "Epoch 15/200\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 157.0680 - val_loss: 109.5408\n",
      "Epoch 16/200\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 144.6832 - val_loss: 113.1956\n",
      "Epoch 17/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 131.8728 - val_loss: 105.8279\n",
      "Epoch 18/200\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 141.5378 - val_loss: 110.5596\n",
      "Epoch 19/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 137.5177 - val_loss: 86.3399\n",
      "Epoch 20/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 128.1724 - val_loss: 83.3921\n",
      "Epoch 21/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 127.1209 - val_loss: 81.1398\n",
      "Epoch 22/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 118.5463 - val_loss: 75.2669\n",
      "Epoch 23/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 114.2270 - val_loss: 71.1032\n",
      "Epoch 24/200\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 115.0473 - val_loss: 68.7677\n",
      "Epoch 25/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 123.3239 - val_loss: 118.5299\n",
      "Epoch 26/200\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 145.8208 - val_loss: 76.3522\n",
      "Epoch 27/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 113.2872 - val_loss: 93.1108\n",
      "Epoch 28/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 118.7928 - val_loss: 60.8624\n",
      "Epoch 29/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 116.9009 - val_loss: 63.1302\n",
      "Epoch 30/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 110.5320 - val_loss: 64.4625\n",
      "Epoch 31/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 102.5000 - val_loss: 58.1582\n",
      "Epoch 32/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 106.1794 - val_loss: 58.4967\n",
      "Epoch 33/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 104.9054 - val_loss: 90.7406\n",
      "Epoch 34/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 115.1508 - val_loss: 62.6577\n",
      "Epoch 35/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 104.8941 - val_loss: 59.6679\n",
      "Epoch 36/200\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 105.1675 - val_loss: 68.8225\n",
      "Epoch 37/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 109.1574 - val_loss: 59.8189\n",
      "Epoch 38/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 100.4937 - val_loss: 57.4182\n",
      "Epoch 39/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 104.7574 - val_loss: 110.5079\n",
      "Epoch 40/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 116.6607 - val_loss: 114.5043\n",
      "Epoch 41/200\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 147.1742 - val_loss: 77.2468\n",
      "Epoch 42/200\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 112.4263 - val_loss: 77.6812\n",
      "Epoch 43/200\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 109.0213 - val_loss: 73.7585\n",
      "Epoch 44/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 122.8854 - val_loss: 75.5645\n",
      "Epoch 45/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 97.9031 - val_loss: 61.1010\n",
      "Epoch 46/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 107.6266 - val_loss: 61.4690\n",
      "Epoch 47/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 101.3946 - val_loss: 59.1270\n",
      "Epoch 48/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 99.4810 - val_loss: 60.7535\n",
      "Epoch 49/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 98.8493 - val_loss: 57.4729\n",
      "Epoch 50/200\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 99.5859 - val_loss: 56.9654\n",
      "Epoch 51/200\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 100.4270 - val_loss: 58.4792\n",
      "Epoch 52/200\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 98.9528 - val_loss: 74.6920\n",
      "Epoch 53/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 105.0104 - val_loss: 57.4915\n",
      "Epoch 54/200\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 102.0967 - val_loss: 87.9959\n",
      "Epoch 55/200\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 117.2263 - val_loss: 67.9045\n",
      "Epoch 56/200\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 121.1343 - val_loss: 79.4303\n",
      "Epoch 57/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 123.8414 - val_loss: 58.2226\n",
      "Epoch 58/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 102.2054 - val_loss: 71.5300\n",
      "Epoch 59/200\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 101.7639 - val_loss: 69.1732\n",
      "Epoch 60/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 113.2606 - val_loss: 62.7936\n",
      "Epoch 61/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 109.7125 - val_loss: 99.5697\n",
      "Epoch 62/200\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 139.3541 - val_loss: 59.6786\n",
      "Epoch 63/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 108.5545 - val_loss: 154.4987\n",
      "Epoch 64/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 143.4195 - val_loss: 56.0456\n",
      "Epoch 65/200\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 113.4860 - val_loss: 60.4946\n",
      "Epoch 66/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 114.5017 - val_loss: 81.6028\n",
      "Epoch 67/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 121.3402 - val_loss: 61.7108\n",
      "Epoch 68/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 108.0774 - val_loss: 80.4592\n",
      "Epoch 69/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 101.9156 - val_loss: 57.8030\n",
      "Epoch 70/200\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 105.1748 - val_loss: 62.6094\n",
      "Epoch 71/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 119.9097 - val_loss: 71.5398\n",
      "Epoch 72/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 126.3473 - val_loss: 201.0365\n",
      "Epoch 73/200\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 140.6207 - val_loss: 55.9292\n",
      "Epoch 74/200\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 113.8886 - val_loss: 58.7602\n",
      "Epoch 75/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 108.5849 - val_loss: 64.0997\n",
      "Epoch 76/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 141.5751 - val_loss: 66.6976\n",
      "Epoch 77/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 112.8582 - val_loss: 202.2217\n",
      "Epoch 78/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 166.5916 - val_loss: 60.4331\n",
      "Epoch 79/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 126.8014 - val_loss: 60.8099\n",
      "Epoch 80/200\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 104.1931 - val_loss: 68.4341\n",
      "Epoch 81/200\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 97.6886 - val_loss: 57.7617\n",
      "Epoch 82/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 118.8865 - val_loss: 86.4924\n",
      "Epoch 83/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 126.2309 - val_loss: 68.0073\n",
      "Epoch 84/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 114.9868 - val_loss: 170.5178\n",
      "Epoch 85/200\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 143.0856 - val_loss: 60.2803\n",
      "Epoch 86/200\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 124.0481 - val_loss: 73.4888\n",
      "Epoch 87/200\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 117.8413 - val_loss: 57.0848\n",
      "Epoch 88/200\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 100.8725 - val_loss: 71.4828\n",
      "Epoch 89/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 101.7300 - val_loss: 62.4043\n",
      "Epoch 90/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 99.0310 - val_loss: 55.5266\n",
      "Epoch 91/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 98.1390 - val_loss: 55.2686\n",
      "Epoch 92/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 103.7557 - val_loss: 55.6949\n",
      "Epoch 93/200\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 112.8533 - val_loss: 55.1733\n",
      "Epoch 94/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 107.7047 - val_loss: 102.3594\n",
      "Epoch 95/200\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 107.2219 - val_loss: 61.7752\n",
      "Epoch 96/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 105.0681 - val_loss: 80.3898\n",
      "Epoch 97/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 118.4098 - val_loss: 58.8642\n",
      "Epoch 98/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 108.3366 - val_loss: 56.1404\n",
      "Epoch 99/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 96.2332 - val_loss: 73.1934\n",
      "Epoch 100/200\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 101.6567 - val_loss: 55.0515\n",
      "Epoch 101/200\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 97.3199 - val_loss: 58.9823\n",
      "Epoch 102/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 105.9512 - val_loss: 54.8983\n",
      "Epoch 103/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 102.6436 - val_loss: 69.6938\n",
      "Epoch 104/200\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 103.9656 - val_loss: 64.0367\n",
      "Epoch 105/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 107.5974 - val_loss: 115.1318\n",
      "Epoch 106/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 133.5225 - val_loss: 100.0074\n",
      "Epoch 107/200\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 128.8169 - val_loss: 56.2839\n",
      "Epoch 108/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 107.0675 - val_loss: 57.2106\n",
      "Epoch 109/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 102.4827 - val_loss: 54.6738\n",
      "Epoch 110/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 94.6557 - val_loss: 90.8010\n",
      "Epoch 111/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 112.0878 - val_loss: 67.1312\n",
      "Epoch 112/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 106.8710 - val_loss: 140.7142\n",
      "Epoch 113/200\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 130.1455 - val_loss: 65.5228\n",
      "Epoch 114/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 96.2029 - val_loss: 73.2092\n",
      "Epoch 115/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 124.9559 - val_loss: 87.9546\n",
      "Epoch 116/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 118.9738 - val_loss: 139.5540\n",
      "Epoch 117/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 115.1698 - val_loss: 81.0927\n",
      "Epoch 118/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 119.2677 - val_loss: 69.4575\n",
      "Epoch 119/200\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 122.9278 - val_loss: 75.2455\n",
      "Epoch 120/200\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 122.3744 - val_loss: 78.2972\n",
      "Epoch 121/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 99.7595 - val_loss: 60.6796\n",
      "Epoch 122/200\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 98.5827 - val_loss: 65.6978\n",
      "Epoch 123/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 94.7919 - val_loss: 74.1701\n",
      "Epoch 124/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 110.9660 - val_loss: 66.1365\n",
      "Epoch 125/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 112.9361 - val_loss: 189.0135\n",
      "Epoch 126/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 156.7789 - val_loss: 129.4316\n",
      "Epoch 127/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 140.7790 - val_loss: 55.9362\n",
      "Epoch 128/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 128.3326 - val_loss: 83.0271\n",
      "Epoch 129/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 102.8779 - val_loss: 119.2583\n",
      "Epoch 130/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 118.7912 - val_loss: 65.7534\n",
      "Epoch 131/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 109.8694 - val_loss: 57.0460\n",
      "Epoch 132/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 107.5171 - val_loss: 128.9274\n",
      "Epoch 133/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 145.4659 - val_loss: 58.4101\n",
      "Epoch 134/200\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 103.4343 - val_loss: 134.3795\n",
      "Epoch 135/200\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 122.9192 - val_loss: 64.4081\n",
      "Epoch 136/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 99.2672 - val_loss: 54.5413\n",
      "Epoch 137/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 91.5428 - val_loss: 100.9927\n",
      "Epoch 138/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 112.7999 - val_loss: 65.8692\n",
      "Epoch 139/200\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 98.9249 - val_loss: 84.8890\n",
      "Epoch 140/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 111.7370 - val_loss: 56.8952\n",
      "Epoch 141/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 110.8697 - val_loss: 70.7284\n",
      "Epoch 142/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 110.2129 - val_loss: 57.3994\n",
      "Epoch 143/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 94.9871 - val_loss: 65.8998\n",
      "Epoch 144/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 95.6635 - val_loss: 62.9670\n",
      "Epoch 145/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 94.5703 - val_loss: 78.6632\n",
      "Epoch 146/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 124.9793 - val_loss: 61.9468\n",
      "Epoch 147/200\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 112.7670 - val_loss: 54.6456\n",
      "Epoch 148/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 97.1580 - val_loss: 55.2245\n",
      "Epoch 149/200\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 94.1111 - val_loss: 62.4983\n",
      "Epoch 150/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 93.5868 - val_loss: 54.9225\n",
      "Epoch 151/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 93.2550 - val_loss: 58.1479\n",
      "Epoch 152/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 94.9447 - val_loss: 57.6486\n",
      "Epoch 153/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 91.9349 - val_loss: 56.9268\n",
      "Epoch 154/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 111.3910 - val_loss: 87.1595\n",
      "Epoch 155/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 118.1971 - val_loss: 62.7025\n",
      "Epoch 156/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 115.5140 - val_loss: 59.1821\n",
      "Epoch 157/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 109.1710 - val_loss: 66.7579\n",
      "Epoch 158/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 101.7633 - val_loss: 104.6717\n",
      "Epoch 159/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 112.6681 - val_loss: 78.9387\n",
      "Epoch 160/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 99.3352 - val_loss: 67.1393\n",
      "Epoch 161/200\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 91.6513 - val_loss: 83.4052\n",
      "Epoch 162/200\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 111.1645 - val_loss: 58.9725\n",
      "Epoch 163/200\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 123.5565 - val_loss: 123.5892\n",
      "Epoch 164/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 145.0282 - val_loss: 120.5359\n",
      "Epoch 165/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 131.1962 - val_loss: 178.4799\n",
      "Epoch 166/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 144.1144 - val_loss: 63.5833\n",
      "Epoch 167/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 104.9221 - val_loss: 71.7340\n",
      "Epoch 168/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 105.6981 - val_loss: 57.0513\n",
      "Epoch 169/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 103.0476 - val_loss: 86.6554\n",
      "Epoch 170/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 108.9425 - val_loss: 101.7461\n",
      "Epoch 171/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 131.2575 - val_loss: 170.8299\n",
      "Epoch 172/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 147.5596 - val_loss: 87.1838\n",
      "Epoch 173/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 145.4862 - val_loss: 55.7198\n",
      "Epoch 174/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 98.6460 - val_loss: 53.6513\n",
      "Epoch 175/200\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 90.5872 - val_loss: 64.5665\n",
      "Epoch 176/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 93.3099 - val_loss: 66.1089\n",
      "Epoch 177/200\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 100.0758 - val_loss: 127.9845\n",
      "Epoch 178/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 138.5100 - val_loss: 66.1294\n",
      "Epoch 179/200\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 122.4463 - val_loss: 54.0949\n",
      "Epoch 180/200\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 94.1067 - val_loss: 100.8982\n",
      "Epoch 181/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 131.7277 - val_loss: 55.2586\n",
      "Epoch 182/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 95.3638 - val_loss: 58.6221\n",
      "Epoch 183/200\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 92.2975 - val_loss: 96.0666\n",
      "Epoch 184/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 118.0280 - val_loss: 122.1866\n",
      "Epoch 185/200\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 130.9889 - val_loss: 71.9980\n",
      "Epoch 186/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 114.8649 - val_loss: 56.2379\n",
      "Epoch 187/200\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 126.1423 - val_loss: 88.6132\n",
      "Epoch 188/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 155.5097 - val_loss: 142.8437\n",
      "Epoch 189/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 141.0735 - val_loss: 53.6433\n",
      "Epoch 190/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 111.8505 - val_loss: 142.5719\n",
      "Epoch 191/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 143.5006 - val_loss: 210.7242\n",
      "Epoch 192/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 166.1889 - val_loss: 54.4185\n",
      "Epoch 193/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 172.7244 - val_loss: 140.6044\n",
      "Epoch 194/200\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 132.1887 - val_loss: 64.9243\n",
      "Epoch 195/200\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 94.2446 - val_loss: 59.3306\n",
      "Epoch 196/200\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 93.8011 - val_loss: 82.7232\n",
      "Epoch 197/200\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 96.7597 - val_loss: 53.7853\n",
      "Epoch 198/200\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 89.4762 - val_loss: 59.2249\n",
      "Epoch 199/200\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 95.4238 - val_loss: 61.5215\n",
      "Epoch 200/200\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 99.6760 - val_loss: 58.4951\n"
     ]
    }
   ],
   "source": [
    "# Label Encoding for categorical features\n",
    "label_encoders = {}\n",
    "for feature in categorical_features:\n",
    "    label_encoders[feature] = LabelEncoder()\n",
    "    df[feature] = label_encoders[feature].fit_transform(df[feature])\n",
    "\n",
    "# One-Hot Encoding for categorical features\n",
    "one_hot_encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "X_cat_encoded = one_hot_encoder.fit_transform(df[categorical_features])\n",
    "\n",
    "# Standard Scaling for numerical features\n",
    "scaler = StandardScaler()\n",
    "X_num_scaled = scaler.fit_transform(df[numerical_features])\n",
    "\n",
    "# Concatenate numerical and categorical features\n",
    "X = np.concatenate((X_cat_encoded, X_num_scaled), axis=1)\n",
    "y = df[target]\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split numerical and categorical features in train and validation sets\n",
    "X_cat_train, X_num_train = X_train[:, :len(categorical_features)], X_train[:, len(categorical_features):]\n",
    "X_cat_val, X_num_val = X_val[:, :len(categorical_features)], X_val[:, len(categorical_features):]\n",
    "\n",
    "# Reshape the numerical data if necessary\n",
    "if len(X_num_train.shape) == 2:\n",
    "    X_num_train = np.expand_dims(X_num_train, axis=2)\n",
    "    X_num_val = np.expand_dims(X_num_val, axis=2)\n",
    "\n",
    "# Define model architecture\n",
    "num_input = Input(shape=(X_num_train.shape[1], X_num_train.shape[2]))\n",
    "rnn_layer = SimpleRNN(64, activation='relu')(num_input)\n",
    "\n",
    "cat_input = Input(shape=(X_cat_train.shape[1],))\n",
    "cat_output = Dense(64, activation='relu')(cat_input)\n",
    "\n",
    "concatenated = Concatenate()([rnn_layer, cat_output])\n",
    "output = Dense(1)(concatenated)\n",
    "\n",
    "model = Model(inputs=[num_input, cat_input], outputs=output)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit([X_num_train, X_cat_train], y_train, epochs=200, batch_size=64,\n",
    "                    validation_data=([X_num_val, X_cat_val], y_val), verbose=1)\n",
    "\n",
    "# Plot the training history\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
