{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_asia.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant features and target\n",
    "features = df[['School enrollment, primary', 'School enrollment, tertiary']]\n",
    "target = df[['Primary completion rate', 'Literacy rate']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_features = MinMaxScaler()\n",
    "scaler_targets = MinMaxScaler()\n",
    "\n",
    "features_scaled = scaler_features.fit_transform(features)\n",
    "targets_scaled = scaler_targets.fit_transform(target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.58842014 0.21671132]]\n",
      "\n",
      " [[0.63683672 0.13792124]]\n",
      "\n",
      " [[       nan 0.24767337]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.48156264        nan]]\n",
      "\n",
      " [[0.55251949        nan]]\n",
      "\n",
      " [[0.65523494 0.39527546]]] [[[0.69307676 0.11332482]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[0.67759809 0.57274289]]\n",
      "\n",
      " [[0.68955742 0.31268412]]\n",
      "\n",
      " [[0.66984253 0.10234607]]\n",
      "\n",
      " [[0.6109772         nan]]\n",
      "\n",
      " [[0.66339359 0.280865  ]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[0.59431857        nan]]\n",
      "\n",
      " [[0.57546179 0.60266108]]\n",
      "\n",
      " [[0.63985804 0.45537042]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[0.58873647        nan]]\n",
      "\n",
      " [[0.6666588  0.14831142]]\n",
      "\n",
      " [[0.69429868 0.03214561]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[0.58568308 0.23078507]]\n",
      "\n",
      " [[0.57581108 0.33133037]]\n",
      "\n",
      " [[0.65869161 0.20127536]]\n",
      "\n",
      " [[0.5713383  0.41666356]]\n",
      "\n",
      " [[0.67478407 0.30179695]]\n",
      "\n",
      " [[0.58541049        nan]]\n",
      "\n",
      " [[0.78345984 0.27324362]]\n",
      "\n",
      " [[0.59119804 0.14867876]]\n",
      "\n",
      " [[0.48251525 0.07634641]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[0.59777221 0.61433619]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[0.62019095 0.29283204]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[0.62788449 0.49307179]]\n",
      "\n",
      " [[0.56774266        nan]]\n",
      "\n",
      " [[0.62110892 0.5148268 ]]\n",
      "\n",
      " [[0.61292931 0.73854404]]\n",
      "\n",
      " [[0.69906196 0.22767677]]\n",
      "\n",
      " [[0.5524977  0.24690634]]\n",
      "\n",
      " [[0.59783428 0.35568529]]\n",
      "\n",
      " [[       nan 0.22014811]]\n",
      "\n",
      " [[0.61338818 0.81828472]]\n",
      "\n",
      " [[0.61032687        nan]]\n",
      "\n",
      " [[0.56694277 0.17750117]]\n",
      "\n",
      " [[0.61629082 0.32418219]]\n",
      "\n",
      " [[0.54242354 0.32400049]]\n",
      "\n",
      " [[0.66780037 0.08036105]]\n",
      "\n",
      " [[0.59686441 0.79732436]]\n",
      "\n",
      " [[0.68202469 0.13337987]]\n",
      "\n",
      " [[0.55603395 0.26638633]]\n",
      "\n",
      " [[       nan 0.11897256]]\n",
      "\n",
      " [[0.58552845        nan]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[0.60135464 0.15822326]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[0.65221797        nan]]\n",
      "\n",
      " [[0.57083735 0.61598554]]\n",
      "\n",
      " [[0.56893392 0.00978599]]\n",
      "\n",
      " [[       nan 0.48860785]]\n",
      "\n",
      " [[0.88174083 0.09476676]]\n",
      "\n",
      " [[0.63810887 0.93024091]]\n",
      "\n",
      " [[0.63168206 0.41102171]]\n",
      "\n",
      " [[0.63876616        nan]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[0.60173476 0.47700444]]\n",
      "\n",
      " [[       nan 0.05103421]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[0.48715474 0.34452053]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[0.59111103 0.14013393]]\n",
      "\n",
      " [[0.70251583        nan]]\n",
      "\n",
      " [[       nan 0.22204702]]\n",
      "\n",
      " [[       nan 0.23754233]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[0.58604088 0.49296927]]\n",
      "\n",
      " [[0.61109837 0.16907511]]\n",
      "\n",
      " [[       nan 0.3553332 ]]\n",
      "\n",
      " [[0.68488365 0.1177662 ]]\n",
      "\n",
      " [[0.69152804 0.01673823]]\n",
      "\n",
      " [[0.59343428 0.28625285]]\n",
      "\n",
      " [[0.61937225 0.21347647]]\n",
      "\n",
      " [[0.55438007 0.13122759]]\n",
      "\n",
      " [[0.72576018 0.4776315 ]]\n",
      "\n",
      " [[0.04006404        nan]]\n",
      "\n",
      " [[0.65457312 0.35221423]]\n",
      "\n",
      " [[0.57808756 0.30494847]]\n",
      "\n",
      " [[0.57258569 0.51058257]]\n",
      "\n",
      " [[0.85341611 0.06759881]]\n",
      "\n",
      " [[       nan 0.24998493]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[0.67222971 0.01675958]]\n",
      "\n",
      " [[0.61566335 0.38746245]]\n",
      "\n",
      " [[0.37368378        nan]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[0.67440924 0.42135209]]\n",
      "\n",
      " [[0.76441963        nan]]\n",
      "\n",
      " [[0.5575022  0.16100499]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[0.6670969  0.39682524]]\n",
      "\n",
      " [[0.68426891 0.12905477]]\n",
      "\n",
      " [[0.4499292  0.06583264]]\n",
      "\n",
      " [[0.60463437 0.3580972 ]]\n",
      "\n",
      " [[0.59405331 0.33729982]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[0.69577919 0.43775174]]\n",
      "\n",
      " [[0.64894228        nan]]\n",
      "\n",
      " [[0.55556889        nan]]\n",
      "\n",
      " [[0.62467177        nan]]\n",
      "\n",
      " [[0.60767271 0.44565317]]\n",
      "\n",
      " [[0.39799768 0.03315389]]\n",
      "\n",
      " [[0.78123192        nan]]\n",
      "\n",
      " [[0.60259108 0.10720833]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[0.55378384 0.2150215 ]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[0.611375   0.74011324]]\n",
      "\n",
      " [[0.54526565 0.16643228]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[0.75025066 0.11287218]]\n",
      "\n",
      " [[       nan 0.17994548]]\n",
      "\n",
      " [[0.48888403 0.31614409]]\n",
      "\n",
      " [[0.58704105 0.15390398]]\n",
      "\n",
      " [[0.65789023        nan]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[       nan 0.14147219]]\n",
      "\n",
      " [[       nan 0.19661732]]\n",
      "\n",
      " [[0.54395059 0.17964963]]\n",
      "\n",
      " [[0.65353832 0.40993029]]\n",
      "\n",
      " [[0.91184495 0.0936484 ]]\n",
      "\n",
      " [[0.68568032 0.09828182]]\n",
      "\n",
      " [[0.58326043 0.46875789]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[0.64607344 0.37199372]]\n",
      "\n",
      " [[0.63303998 0.37124875]]\n",
      "\n",
      " [[0.67319238 0.17608693]]\n",
      "\n",
      " [[       nan 0.45853807]]\n",
      "\n",
      " [[0.65285401        nan]]\n",
      "\n",
      " [[0.60857151 0.71355931]]\n",
      "\n",
      " [[0.55063915 0.0909123 ]]\n",
      "\n",
      " [[0.5563232  0.34122992]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[0.64575979 0.29547654]]\n",
      "\n",
      " [[0.80609515 0.09636643]]\n",
      "\n",
      " [[       nan 0.09161871]]\n",
      "\n",
      " [[0.62350568 0.18758943]]\n",
      "\n",
      " [[       nan 0.4493883 ]]\n",
      "\n",
      " [[       nan 0.10640719]]\n",
      "\n",
      " [[0.65356594 0.27826741]]\n",
      "\n",
      " [[0.57403722        nan]]\n",
      "\n",
      " [[0.56934077 0.13599633]]\n",
      "\n",
      " [[0.47514488 0.07579452]]\n",
      "\n",
      " [[0.61186113 0.53384425]]\n",
      "\n",
      " [[0.46703908 0.06514902]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[0.60700036 0.05598376]]\n",
      "\n",
      " [[0.69773256 0.12830152]]\n",
      "\n",
      " [[0.77246847 0.1092651 ]]\n",
      "\n",
      " [[0.60328735 0.12798702]]\n",
      "\n",
      " [[0.53329049 0.19328673]]\n",
      "\n",
      " [[0.80517897        nan]]\n",
      "\n",
      " [[0.62816421        nan]]\n",
      "\n",
      " [[0.62698955 0.0961665 ]]\n",
      "\n",
      " [[0.58597304 0.14702283]]\n",
      "\n",
      " [[0.66885535        nan]]\n",
      "\n",
      " [[0.68608075 0.31463226]]\n",
      "\n",
      " [[0.55890719 0.46629228]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[0.61177769 0.08195362]]\n",
      "\n",
      " [[0.76452705 0.17091356]]\n",
      "\n",
      " [[0.58631269 0.42288766]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[0.66971898 0.27200436]]\n",
      "\n",
      " [[0.62419624 0.37820163]]\n",
      "\n",
      " [[0.630058   0.25925051]]\n",
      "\n",
      " [[0.56095471 0.15001877]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[0.60533767 0.37683798]]\n",
      "\n",
      " [[0.78071206        nan]]\n",
      "\n",
      " [[0.58478927 0.28717815]]\n",
      "\n",
      " [[0.62745335 0.30038074]]\n",
      "\n",
      " [[0.41098753 0.06689567]]\n",
      "\n",
      " [[0.58628288 0.1841193 ]]\n",
      "\n",
      " [[0.61043352 0.341124  ]]\n",
      "\n",
      " [[       nan 0.12175494]]\n",
      "\n",
      " [[0.58519231 0.12894084]]\n",
      "\n",
      " [[0.53444611 0.28392114]]\n",
      "\n",
      " [[0.7277033  0.18468422]]\n",
      "\n",
      " [[0.61628981 0.04240169]]\n",
      "\n",
      " [[0.69358502 0.24143548]]\n",
      "\n",
      " [[       nan 0.0657292 ]]\n",
      "\n",
      " [[0.6902087  0.20791187]]\n",
      "\n",
      " [[0.58306236 0.12376122]]\n",
      "\n",
      " [[0.66711594 0.2048614 ]]\n",
      "\n",
      " [[0.67730944 0.29970431]]\n",
      "\n",
      " [[0.47665914 0.06841449]]\n",
      "\n",
      " [[0.59189807        nan]]\n",
      "\n",
      " [[0.51563979 0.04369806]]\n",
      "\n",
      " [[0.58917278 0.49262512]]\n",
      "\n",
      " [[0.58499668 0.37612223]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[0.55919293 0.13293534]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[0.62094299 0.56628714]]\n",
      "\n",
      " [[0.58885342 0.14125825]]\n",
      "\n",
      " [[0.64124094        nan]]\n",
      "\n",
      " [[0.59389321 0.20877384]]\n",
      "\n",
      " [[0.5901339  0.07708707]]\n",
      "\n",
      " [[0.69991631        nan]]\n",
      "\n",
      " [[       nan 0.05615468]]\n",
      "\n",
      " [[0.58534324 0.28520355]]\n",
      "\n",
      " [[0.48525523 0.09864892]]\n",
      "\n",
      " [[0.65968368 0.1691229 ]]\n",
      "\n",
      " [[0.54719432 0.24659706]]\n",
      "\n",
      " [[0.74400371 0.20255372]]\n",
      "\n",
      " [[0.70258749 0.14004784]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[       nan 0.23324885]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[       nan 0.23254878]]\n",
      "\n",
      " [[0.50340146 0.3452018 ]]\n",
      "\n",
      " [[0.60276284 0.3807383 ]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[0.71780054 0.14311572]]\n",
      "\n",
      " [[0.62631    0.27247877]]\n",
      "\n",
      " [[0.65787208 0.16166338]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[0.53619468 0.31887079]]\n",
      "\n",
      " [[0.59519952 0.29300097]]\n",
      "\n",
      " [[0.52981204 0.12577928]]\n",
      "\n",
      " [[0.54341923        nan]]\n",
      "\n",
      " [[0.73172407 0.23379317]]\n",
      "\n",
      " [[       nan 0.11292879]]\n",
      "\n",
      " [[0.60569899 0.33102426]]\n",
      "\n",
      " [[0.58878617 0.50361817]]\n",
      "\n",
      " [[0.6054495  0.18871632]]\n",
      "\n",
      " [[0.44411011 0.02038377]]\n",
      "\n",
      " [[0.5424852         nan]]\n",
      "\n",
      " [[0.60981402 0.21031561]]\n",
      "\n",
      " [[       nan 0.55198251]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[0.69152066 0.16404862]]\n",
      "\n",
      " [[0.61561859 0.35635847]]\n",
      "\n",
      " [[0.71088934 0.08915055]]\n",
      "\n",
      " [[0.5698363         nan]]\n",
      "\n",
      " [[0.75850513 0.13775141]]\n",
      "\n",
      " [[0.57891453 0.44774132]]\n",
      "\n",
      " [[0.59995614        nan]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[0.50848303 0.03741001]]\n",
      "\n",
      " [[       nan        nan]]\n",
      "\n",
      " [[       nan 0.09776076]]\n",
      "\n",
      " [[0.73041198 0.05964721]]\n",
      "\n",
      " [[0.59650357 0.3263528 ]]\n",
      "\n",
      " [[       nan 0.09611705]]\n",
      "\n",
      " [[0.58791944 0.14717943]]\n",
      "\n",
      " [[0.62163641        nan]]\n",
      "\n",
      " [[0.6120852  0.67367113]]\n",
      "\n",
      " [[0.58792884 0.25874313]]\n",
      "\n",
      " [[0.75425595 0.16073977]]\n",
      "\n",
      " [[0.6913236  0.11718372]]\n",
      "\n",
      " [[0.83008178 0.08255617]]]\n"
     ]
    }
   ],
   "source": [
    "# Split data into training, validation, and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_scaled, targets_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.SimpleRNN(50, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    tf.keras.layers.Dense(2)  # 2 đầu ra cho 2 cột cần dự đoán\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "33/33 [==============================] - 1s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 11/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 12/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 13/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 14/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 15/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 16/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 17/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 18/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 19/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: nan - val_loss: nan\n",
      "Epoch 20/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: nan - val_loss: nan\n",
      "Epoch 21/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 22/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 23/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 24/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: nan - val_loss: nan\n",
      "Epoch 25/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: nan - val_loss: nan\n",
      "Epoch 26/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 27/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 28/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 29/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 30/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 31/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: nan - val_loss: nan\n",
      "Epoch 32/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: nan - val_loss: nan\n",
      "Epoch 33/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 34/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 35/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 36/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 37/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 38/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 39/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 40/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 41/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 42/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 43/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 44/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 45/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 46/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 47/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 48/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 49/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 50/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1ea6f96bc50>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 997us/step\n"
     ]
    }
   ],
   "source": [
    "predicted_values_scaled = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_values = scaler_targets.inverse_transform(predicted_values_scaled)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
